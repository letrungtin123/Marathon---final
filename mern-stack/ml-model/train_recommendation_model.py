# Embedding + Deep Learning


# train_recommendation_model.py
import pandas as pd
import numpy as np
from pymongo import MongoClient
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder
import joblib
import os
from dotenv import load_dotenv

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env (ch·ª©a MONGO_URI)
load_dotenv()
mongo_uri = os.getenv("MONGO_URI")

if not mongo_uri:
    raise Exception("‚ùå MONGO_URI not found in .env")

# K·∫øt n·ªëi MongoDB v√† l·∫•y collection orders
client = MongoClient(mongo_uri)
db = client["test"]
orders = db["orders"]

# Tr√≠ch xu·∫•t d·ªØ li·ªáu user-product-quantity t·ª´ ƒë∆°n h√†ng
data = []
for order in orders.find():
    user_raw = order.get("userId")
    if not user_raw:
        continue

    user = str(user_raw["$oid"]) if isinstance(user_raw, dict) else str(user_raw)

    for p in order.get("products", []):
        product_raw = p.get("productId")
        if not product_raw:
            continue
        product = str(product_raw["$oid"]) if isinstance(product_raw, dict) else str(product_raw)
        quantity = p.get("quantity", 1)
        data.append((user, product, quantity))

# Chuy·ªÉn th√†nh DataFrame
df = pd.DataFrame(data, columns=["user", "product", "quantity"])

if df.empty:
    sample = orders.find_one()
    print("üß™ order sample:", sample)
    raise Exception("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆°n h√†ng ƒë·ªÉ hu·∫•n luy·ªán!")

# Encode user & product IDs
user_encoder = LabelEncoder()
product_encoder = LabelEncoder()

df["user_id"] = user_encoder.fit_transform(df["user"])
df["product_id"] = product_encoder.fit_transform(df["product"])
df["interaction"] = df["quantity"]

# L∆∞u l·∫°i encoder
joblib.dump(user_encoder, "user_encoder.pkl")
joblib.dump(product_encoder, "product_encoder.pkl")

# S·ªë l∆∞·ª£ng unique users v√† products
n_users = df["user_id"].nunique()
n_products = df["product_id"].nunique()

# X√¢y d·ª±ng m√¥ h√¨nh Deep Learning ƒë∆°n gi·∫£n
user_input = Input(shape=(1,))
product_input = Input(shape=(1,))
user_embedding = Embedding(input_dim=n_users, output_dim=50)(user_input)
product_embedding = Embedding(input_dim=n_products, output_dim=50)(product_input)

user_vec = Flatten()(user_embedding)
product_vec = Flatten()(product_embedding)
concat = Concatenate()([user_vec, product_vec])
dense1 = Dense(128, activation="relu")(concat)
dense2 = Dense(64, activation="relu")(dense1)
output = Dense(1)(dense2)

model = Model(inputs=[user_input, product_input], outputs=output)
model.compile(optimizer=Adam(0.001), loss="mse")

# Hu·∫•n luy·ªán m√¥ h√¨nh
model.fit(
    [df["user_id"], df["product_id"]],
    df["interaction"],
    epochs=10,
    batch_size=32,
    validation_split=0.1
)

# L∆∞u m√¥ h√¨nh
model.save("recommendation_model.h5")
print("‚úÖ ƒê√£ hu·∫•n luy·ªán xong m√¥ h√¨nh v√† l∆∞u v√†o recommendation_model.h5")

# train_recommendation_model.py
# C√¥ng ngh·ªá, th∆∞ vi·ªán s·ª≠ d·ª•ng
# pandas, numpy: x·ª≠ l√Ω d·ªØ li·ªáu b·∫£ng, m·∫£ng.

# pymongo: k·∫øt n·ªëi MongoDB l·∫•y d·ªØ li·ªáu ƒë∆°n h√†ng.

# tensorflow.keras: x√¢y d·ª±ng m√¥ h√¨nh deep learning (embedding + fully connected layers).

# sklearn.preprocessing.LabelEncoder: m√£ h√≥a user v√† product th√†nh s·ªë nguy√™n.

# joblib: l∆∞u b·ªô encoder v√†o file.

# dotenv: load bi·∫øn m√¥i tr∆∞·ªùng.

# Ph∆∞∆°ng th·ª©c ch√≠nh
# K·∫øt n·ªëi MongoDB l·∫•y data t·ª´ collection orders.

# Chu·∫©n b·ªã DataFrame df g·ªìm 3 c·ªôt: user, product, quantity (l∆∞·ª£ng mua).

# Label encode user v√† product (chuy·ªÉn string id th√†nh integer).

# X√¢y d·ª±ng m√¥ h√¨nh DL:

# Embedding cho user v√† product (size embedding=50).

# Flatten embedding ‚Üí concat ‚Üí Dense 128 ‚Üí Dense 64 ‚Üí Output 1 (s·ªë l∆∞·ª£ng t∆∞∆°ng t√°c d·ª± ƒëo√°n).

# Compile v·ªõi optimizer Adam v√† loss MSE.

# Train 10 epochs v·ªõi batch size 32, validation 10%.

# L∆∞u m√¥ h√¨nh v√† encoder ra file.

# C√°ch ho·∫°t ƒë·ªông
# M·ª•c ƒë√≠ch: H·ªçc m·ªôt m√¥ h√¨nh ƒë·ªÉ d·ª± ƒëo√°n m·ª©c ƒë·ªô t∆∞∆°ng t√°c gi·ªØa user v√† product d·ª±a tr√™n embedding (ƒë·∫∑c tr∆∞ng h√≥a users v√† products th√†nh vector) qua m·∫°ng neural.

# -------------------------------Explain-------------------------------------

# ƒëo·∫°n code n√†y t·ª± x√¢y v√† hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh t·ª´ ƒë·∫ßu (from scratch), kh√¥ng d√πng m√¥ h√¨nh hay embedding ‚Äúƒë√£ h·ªçc s·∫µn‚Äù. Hai l·ªõp Embedding ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n v√† ƒë∆∞·ª£c c·∫≠p nh·∫≠t trong qu√° tr√¨nh training.

# Input (ƒë·∫ßu v√†o) cho m√¥ h√¨nh khi hu·∫•n luy·ªán:

# user_id (ƒë√£ LabelEncode t·ª´ user): tensor shape (batch_size, 1)

# product_id (ƒë√£ LabelEncode t·ª´ product): tensor shape (batch_size, 1)

# Nh√£n/target: interaction = quantity (s·ªë l∆∞·ª£ng mua) ‚Äî d√πng ƒë·ªÉ t√≠nh loss MSE.

# Output (ƒë·∫ßu ra) c·ªßa m√¥ h√¨nh:

# M·ªôt gi√° tr·ªã th·ª±c (scalar) shape (batch_size, 1) d·ª± ƒëo√°n m·ª©c ƒë·ªô t∆∞∆°ng t√°c gi·ªØa user v√† product (·ªü ƒë√¢y l√† s·ªë l∆∞·ª£ng/ƒë·ªô m·∫°nh t∆∞∆°ng t√°c).
# c√≥ th·ªÉ di·ªÖn gi·∫£i l√† ‚Äúƒëi·ªÉm ph√π h·ª£p‚Äù ho·∫∑c ‚Äúk·ª≥ v·ªçng s·ªë l∆∞·ª£ng mua‚Äù.

# T√≥m t·∫Øt lu·ªìng:
# (user_id, product_id) -> Embedding(user) + Embedding(product) -> concat -> Dense(128) -> Dense(64) -> Dense(1) -> d·ª± ƒëo√°n interaction
# Sau khi train xong, code l∆∞u:

# m√¥ h√¨nh: recommendation_model.h5

# encoder: user_encoder.pkl, product_encoder.pkl ƒë·ªÉ map ID g·ªëc ‚Üî ch·ªâ s·ªë embedding.

# G·ª£i √Ω ng·∫Øn: n·∫øu quantity l·ªách nhi·ªÅu, c√¢n nh·∫Øc log-transform ho·∫∑c chu·∫©n h√≥a; n·∫øu ch·ªâ c√≥ implicit feedback, c√≥ th·ªÉ chuy·ªÉn nh√£n sang {0,1} v√† d√πng loss ph√π h·ª£p (BCE, BPR, v.v.)